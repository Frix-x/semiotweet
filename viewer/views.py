#-*- coding: utf-8 -*-
from __future__ import print_function
from __future__ import absolute_import
from builtins import str
from django.utils import timezone
from django.http import HttpResponse,Http404
from django.shortcuts import render,redirect
from .models import Tweet,User,LdaModel
import random
import string
import time
import pytz
from datetime import datetime
from django.db.models import Max
from django.db import connection #for direct SQL requests

import json
import ast # convert string to list
import math

from .extraction import *
from .semanticAnalysis import *

#==============================#
#=========== OTHERS ===========#
#==============================#

def handler404(request,typed):
    """A basic 404 error handler"""
    response = render(request,'404.html', {"typed":typed})
    response.status_code = 404
    return response


def home(request):
    """Redirect to the home page : global statistics"""
    global requestToGetSources

    # Getting tweets' sources
    cursor = connection.cursor()
    try:
        cursor.execute(requestToGetSources)
    except BaseException as error:
        print("view home ; error : ", error)
        return render(request,'home.html',{'error':'No data yet ; click on "Get the data"'})
    res = cursor.fetchall()

    sources = []
    num = []
    for (s,n) in res:
        sources.append(s)
        num.append(n)

    # Keeping only the most commons stats
    if len(sources)>5 :
        sources = sources[0:5]
        sources.append(u"Others")
        num[5] = sum(num[5:-1])
        num = num[0:6]

    # JSON Formating
    sources = json.dumps(sources)
    num = json.dumps(num)

    # Getting the number of tweets for each user
    try:
        cursor.execute("SELECT u.name, COUNT(t.id) AS nbTweets FROM viewer_tweet t, viewer_user u WHERE u.id = t.user_id_id GROUP BY u.name ORDER BY nbTweets DESC")
    except BaseException:
        print("view home ; error : ",error)
        return render(request,'home.html',{"error":"No data yet ; click on 'Get the data'"})
    res = cursor.fetchall()
    politics = []
    nbTweets = []
    for (p,n) in res:
        politics.append(p)
        nbTweets.append(n)
    # JSON Formating
    politics = json.dumps(politics)
    nbTweets = json.dumps(nbTweets)

    # Getting the date of the last tweet
    lastTweet = Tweet.objects.aggregate(Max('id'))
    lastId = lastTweet["id__max"]
    try:
        maj = Tweet.objects.get(id=lastId).created_at
    except BaseException as e:
        maj = False
        print(e)
        pass

    return render(request,'home.html',{'sources': sources,
                                       'num': num,
                                       'politics': politics,
                                       'nbTweets': nbTweets,
                                       'maj' : maj})

def generalOverview(request):
    """Redirect to the words page : analysis around words used by politics"""
    # Getting tweets' sources
    cursor = connection.cursor()
    # Getting the number of tweets for each user
    try:
        cursor.execute("SELECT u.name, COUNT(t.id) AS nbTweets FROM viewer_tweet t, viewer_user u WHERE u.id = t.user_id_id GROUP BY u.name ORDER BY nbTweets DESC")
    except BaseException:
        print("view generalOverview ; error : ",error)
        return render(request,'generalOverview.html',{'error':'No data yet ; click on "Get the data"'})
    res = cursor.fetchall()
    politics = []
    nbTweets = []
    for (p,n) in res:
        politics.append(p)
        nbTweets.append(n)
    # JSON Formating
    politics = json.dumps(politics)
    nbTweets = json.dumps(nbTweets)

    # tokenArray and lemmaArray are stored as a string : we need to get the lists back
    allTokenArray = Tweet.objects.values('tokenArray')
    words = [ast.literal_eval(t["tokenArray"]) for t in allTokenArray]
    allLemmaArray = Tweet.objects.values('lemmaArray')
    lemmes = [ast.literal_eval(t["lemmaArray"]) for t in allLemmaArray]

    # words is a JSON list of dict like : {"word":"foo", "occur":42}
    words = json.dumps(toJsonForGraph(countWords(words)))
    lemmes = json.dumps(toJsonForGraph(countWords(lemmes)))

    #Get hours distribution of all tweets
    try:
        cursor.execute("SELECT DISTINCT created_at AS timePosted FROM viewer_tweet ORDER BY timePosted")
    except BaseException as error:
        print("displayInfo() ; error : ", error)
        return render(request,'home.html',{'politics': politics, 'nbTweets': nbTweets, 'words': words, 'lemmes': lemmes})
    res = cursor.fetchall()
    hours =[0]*24
    for (time,) in res:
        hours[time.time().hour]+=1

    # Get LDA topics distribution for bubble Graph
    try :
        ldamodel = pickle.loads(LdaModel.objects.get(user_id=0).ldamodel)
    except BaseException as error:
        print("displayInfo() ; error : ", error)
        return render(request,'generalOverview.html',{'politics': politics, 'nbTweets': nbTweets, 'words': words, 'lemmes': lemmes, 'hours': hours})

    topics = ldamodel.show_topics(num_topics=10, num_words=8, log=False, formatted=False)
    bubblesJson = {"label":"Topics","amount":50,"children":[]}
    for index, topic in enumerate(topics):
        bubblesJson["children"].append({"label":topic[0],"amount":10,"children":[]})
        for word in topic[1]:
            bubblesJson["children"][index]["children"].append({"label":word[0],"amount":math.floor(300*word[1])})
    bubblesJson = json.dumps(bubblesJson,default=str)

    return render(request,'generalOverview.html',{'politics': politics, 'nbTweets': nbTweets, 'words': words, 'lemmes': lemmes, 'hours': hours, 'bubblesJson': bubblesJson})

def comparison(request):
    """Redirect to the comparison form page : compare two politics"""
    global screen_nameToExtract
    users = {}
    candidats = []
    wordsList = []
    lemmesList = []
    sourcesList = []
    numList = []
    hoursList = []
    for screen_name in screen_nameToExtract:
        users[screen_name] = returnUser(screen_name, toClean=False)

    if request.method == 'GET' and 'candidat1' in request.GET and 'candidat2' in request.GET:
        candidats.append(request.GET['candidat1'])
        candidats.append(request.GET['candidat2'])

        if candidats[0] in screen_nameToExtract and candidats[1] in screen_nameToExtract and candidats[0] != candidats[1]:
            for candidat in candidats:
                idUser = users[candidat]["id"]
                allTokenArray = Tweet.objects.filter(user_id=idUser).values('tokenArray')
                allLemmaArray = Tweet.objects.filter(user_id=idUser).values('lemmaArray')
                # tokenArray is stored as a string : we need to get the list back
                words = [ast.literal_eval(t["tokenArray"]) for t in allTokenArray]
                lemmes = [ast.literal_eval(t["lemmaArray"]) for t in allLemmaArray]
                # words is a JSON list of dict like : {"word":"foo", "occur":42}
                wordsList.append(json.dumps(toJsonForGraph(countWords(words))))
                lemmesList.append(json.dumps(toJsonForGraph(countWords(lemmes))))
                # Sources of Tweets
                global requestToGetSources
                cursor = connection.cursor()
                try:
                    cursor.execute("SELECT DISTINCT source, COUNT(source) AS nb FROM viewer_tweet WHERE user_id_id ='"+ str(idUser)+"' GROUP BY source ORDER BY nb DESC")
                except BaseException as error:
                    return render(request,'comparison.html', {'users':users})
                res = cursor.fetchall()
                sources = []
                num = []
                for (s,n) in res:
                    sources.append(s)
                    num.append(n)
                # Keeping only the most commons stats
                if len(sources)>5 :
                    sources = sources[0:5]
                    sources.append(u"Others")
                    num[5] = sum(num[5:-1])
                    num = num[0:6]
                # JSON Formating
                sourcesList.append(json.dumps(sources))
                numList.append(json.dumps(num))

                #Get hours distribution of user's tweets
                try:
                    cursor.execute("SELECT DISTINCT created_at AS timePosted FROM viewer_tweet WHERE user_id_id ='"+ str(idUser)+"' ORDER BY timePosted")
                except BaseException as error:
                    return render(request,'comparison.html', {'users':users})
                res = cursor.fetchall()
                hours =[0]*24
                for (time,) in res:
                    hours[time.time().hour]+=1
                hoursList.append(hours)
            return render(request,'comparison.html', {  "users":users,
                                                        "candidats":candidats,
                                                        "wordsList": wordsList,
                                                        "lemmesList": lemmesList,
                                                        "sourcesList": sourcesList,
                                                        "numList": numList,
                                                        "hoursList": hoursList})
        else:
            return render(request,'comparison.html', {'users':users})


    else:
        return render(request,'comparison.html', {'users':users})
    return render(request,'comparison.html', {'users':users})


def displayInfo(request,screen_name):
    """Display all the tweets for a user
    Requests the Twitter API directly and search for the most common words"""
    userInfo = returnUser(screen_name,toClean=False)
    success = True
    if not(userInfo): # If the user doesn't exist
        success = False
        return render(request,'displayInfo.html',{"success" : success})

    userInfo["profile_image_url_https"] = userInfo["profile_image_url_https"].replace('_normal.jpg','.jpg')

    # Most common words said by the user
    idUser = userInfo["id"]
    allTokenArray = Tweet.objects.filter(user_id=idUser).values('tokenArray')
    allLemmaArray = Tweet.objects.filter(user_id=idUser).values('lemmaArray')

    # tokenArray is stored as a string : we need to get the list back
    words = [ast.literal_eval(t["tokenArray"]) for t in allTokenArray]
    lemmes = [ast.literal_eval(t["lemmaArray"]) for t in allLemmaArray]

    # words is a JSON list of dict like : {"word":"foo", "occur":42}
    words = json.dumps(toJsonForGraph(countWords(words)))
    lemmes = json.dumps(toJsonForGraph(countWords(lemmes)))

    # Sources of Tweets
    global requestToGetSources
    cursor = connection.cursor()
    try:
        cursor.execute("SELECT DISTINCT source, COUNT(source) AS nb FROM viewer_tweet WHERE user_id_id ='"+ str(idUser)+"' GROUP BY source ORDER BY nb DESC")
    except BaseException as error:
        print("displayInfo() ; error : ", error)
        return render(request,'home.html',{"success": success})
    res = cursor.fetchall()
    sources = []
    num = []
    for (s,n) in res:
        sources.append(s)
        num.append(n)

    # Keeping only the most commons stats
    if len(sources)>5 :
        sources = sources[0:5]
        sources.append(u"Others")
        num[5] = sum(num[5:-1])
        num = num[0:6]

    # JSON Formating
    sources = json.dumps(sources)
    num = json.dumps(num)

    #Get hours distribution of user's tweets
    try:
        cursor.execute("SELECT DISTINCT created_at AS timePosted FROM viewer_tweet WHERE user_id_id ='"+ str(idUser)+"' ORDER BY timePosted")
    except BaseException as error:
        print("displayInfo() ; error : ", error)
        return render(request,'home.html',{"success":success})
    res = cursor.fetchall()
    hours =[0]*24
    for (time,) in res:
        hours[time.time().hour]+=1

    return render(request,'displayInfo.html',{"userInfo" : userInfo,
                                              "words" : words,
                                              "lemmes" : lemmes,
                                              "hours" : hours,
                                              "sources" : sources,
                                              "screen_name" : screen_name,
                                              "num" : num,
                                              "success" : success})



#==============================#
#===========  DATA  ===========#
#==============================#

def getData(request):
    """Save the latest tweets from all the users defined in screen_nameToExtract
     and stores info about a users in the database not already done"""
    global screen_nameToExtract

    # SAVING USERS
    for screen_name in screen_nameToExtract:
        print ("GET User info : "+screen_name)
        userInfo = returnUser(screen_name,toClean=True)

        if not(userInfo): #If the user doesn't exist
            success = False
            error = "The user doesn't exist"
            return render(request,'getData.html',{"success" : success,
                                                  "error" : error})

        # Saving the user
        success = saveUser(userInfo)
        if not(success):
            error = "Error during saving in DB"
            return render(request,'getData.html',{"success" : success,
                                                  "error" : error})

    # SAVING TWEETS
    lastId = 0
    nbTweets = 0 # number of extracted tweets

    for screen_name in screen_nameToExtract:
        print ("GET user tweets : "+screen_name)
        try:
            idUser = User.objects.filter(screen_name=screen_name).values('id')[0]["id"]
        except SomeModel.DoesNotExist: # If the user does not exist
            continue # continue with the next user

        try: # We try to get the id of the user's last tweet
            lastTweet = Tweet.objects.filter(user_id=idUser).aggregate(Max('id'))
            lastId = lastTweet["id__max"]
            # maj = Tweet.objects.get(id=lastId).created_at
        except SomeModel.DoesNotExist:
            lastId = 0 # No tweet in the data base

        # The important request here : returns the new tweets from the Twitter API
        tweets = returnTweetsMultiple(screen_name,lastId)

        lenghtTweets = len(tweets)
        nbTweets += lenghtTweets

        # After this operations each tweet in tweets contains 2 more fields : 'tokenArray' & 'lemmaArray'
        tweets = tokenizeAndLemmatizeTweets(tweets)

        # Saving tweets in database
        userFrom = User.objects.get(screen_name=screen_name)
        nbTweetsSaved = 1; #counter for printing
        for t in tweets:
            print ("Saving {1} tweets from {0} ; {2:0.2f} %".format(screen_name,lenghtTweets,nbTweetsSaved/lenghtTweets*100))
            success = saveTweet(t,userFrom) and success
            nbTweetsSaved += 1;

    # MAKING LDA MODELS
    for screen_name in screen_nameToExtract:
        print ("MAKING LDA MODEL FOR : "+screen_name)
        try:
            idUser = User.objects.filter(screen_name=screen_name).values('id')[0]["id"]
        except SomeModel.DoesNotExist: # If the user does not exist
            continue # continue with the next user
        makeLdaModel(idUser)
    print("MAKING GLOBAL LDA MODEL...")
    makeLdaModel(0)

    print("\nEVERYTHING DONE !")
    return render(request,'getData.html',{"success" : success,
                                          "nbTweets" : nbTweets,
                                          "screen_nameToExtract": screen_nameToExtract})


#==============================#
#===== SEMANTIC NETWORK =======#
#==============================#

def displayNetwork(request):
    """Display a force network with users linked to their matching keywords"""

    return render(request,'displayNetwork.html',{"success": True})
